# -*- coding: utf-8 -*-
"""STT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vlykcA13_QhhLlW6yaR2KsFJCLBREtYF
"""

!pip install -q datasets huggingface_hub
!huggingface-cli login

!pip install --upgrade datasets fsspec pyarrow

from datasets import load_dataset, Audio
import librosa
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from transformers import AutoTokenizer, AutoProcessor

import shutil
import os

new_cache_dir = "/content/hf_cache_clean"

if os.path.exists(new_cache_dir):
    shutil.rmtree(new_cache_dir)
os.makedirs(new_cache_dir, exist_ok=True)

os.environ["HF_DATASETS_CACHE"] = new_cache_dir
os.environ["HF_HUB_CACHE"] = new_cache_dir

!rm -rf /root/.cache/huggingface/datasets/*

!ls -lah /root/.cache/huggingface/datasets/

from datasets import load_dataset

# Load dataset seperti biasa
dataset = load_dataset(
    "mozilla-foundation/common_voice_12_0",
    "id",
    split="train+validation",  # Ambil data train + validasi
    download_mode="force_redownload",
    token=True
)

# Split data jadi 80% train, 10% validasi, 10% test
split1 = dataset.train_test_split(test_size=0.2, seed=42)
dtrain = split1["train"]
dtemp = split1["test"]

split2 = dtemp.train_test_split(test_size=0.5, seed=42)
dvalid = split2["train"]
dtest = split2["test"]

from collections import Counter

sample_rates = []
invalid_indices = []

for i in range(len(dtrain)):
    try:
        sr = dtrain[i]["audio"]["sampling_rate"]
        sample_rates.append(sr)
    except Exception as e:
        print(f"Audio rusak di index {i}: {e}")
        invalid_indices.append(i)

counter = Counter(sample_rates)
print("\nüìä Sample rate yang ditemukan:")
for sr, count in counter.items():
    print(f"{sr} Hz: {count} contoh")

print(f"\nüßπ Jumlah audio rusak: {len(invalid_indices)}")

if len(invalid_indices) > 0:
    dtrain = dtrain.select([i for i in range(len(dtrain)) if i not in invalid_indices])
    print(f"‚úÖ Dataset dibersihkan. Ukuran baru: {len(dtrain)} sample.")

from google.colab import drive
drive.mount('/content/drive')

import torch
import torchaudio
from datasets import load_dataset, Audio
import datasets

# 2. Split data: 80% train, 10% validasi, 10% test
split1 = dataset.train_test_split(test_size=0.2, seed=42)
dtrain = split1["train"]
dtemp = split1["test"]

split2 = dtemp.train_test_split(test_size=0.5, seed=42)
dvalid = split2["train"]
dtest = split2["test"]

# 3. Pastikan semua audio di-cast ke 48kHz dulu (untuk standarisasi)
dtrain = dtrain.cast_column("audio", Audio(sampling_rate=48_000))
dvalid = dvalid.cast_column("audio", Audio(sampling_rate=48_000))
dtest = dtest.cast_column("audio", Audio(sampling_rate=48_000))

# 4. Fungsi resample dari 48kHz ke 16kHz
def resample(example):
    array = example["audio"]["array"]
    resampler = torchaudio.transforms.Resample(orig_freq=48_000, new_freq=16_000)
    waveform = torch.tensor(array, dtype=torch.float32).unsqueeze(0)
    resampled = resampler(waveform).squeeze(0)
    example["audio"]["array"] = resampled.numpy()
    example["audio"]["sampling_rate"] = 16_000
    return example

# 5. Terapkan ke semua split
dtrain = dtrain.map(resample)
dtrain = dtrain.cast_column("audio", Audio(sampling_rate=16000))

dvalid = dvalid.map(resample)
dvalid = dvalid.cast_column("audio", Audio(sampling_rate=16000))

dtest = dtest.map(resample)
dtest = dtest.cast_column("audio", Audio(sampling_rate=16000))

# 6. (Opsional) Simpan untuk digunakan kembali tanpa proses ulang
dtrain.save_to_disk("/content/common_voice_id_train_16k.arrow")
dvalid.save_to_disk("/content/common_voice_id_valid_16k.arrow")
dtest.save_to_disk("/content/common_voice_id_test_16k.arrow")

print("‚úÖ Semua data berhasil di-resample dan disimpan.")

from datasets import load_from_disk

dtrain = load_from_disk("/content/common_voice_id_train_16k.arrow")
dvalid = load_from_disk("/content/common_voice_id_valid_16k.arrow")
dtest = load_from_disk("/content/common_voice_id_test_16k.arrow")

def filter_problematic(example):
    audio = example["audio"]
    text = example["sentence"]

    # Cek durasi
    duration = len(audio["array"]) / audio["sampling_rate"]

    # Buang kalau:
    return (
        audio["array"].size > 0 and
        text.strip() != "" and
        len(text.strip()) >= 1 and
        duration >= 0.5
    )

# Terapkan filter ke dataset yang sudah di-resample
dtrain = dtrain.filter(filter_problematic)
dvalid = dvalid.filter(filter_problematic)
dtest = dtest.filter(filter_problematic)

dtrain.save_to_disk("/content/common_voice_train_clean.arrow")
dvalid.save_to_disk("/content/common_voice_valid_clean.arrow")
dtest.save_to_disk("/content/common_voice_test_clean.arrow")

dtrain = load_from_disk("/content/common_voice_train_clean.arrow")
dvalid = load_from_disk("/content/common_voice_valid_clean.arrow")
dtest = load_from_disk("/content/common_voice_test_clean.arrow")

import numpy as np
from datasets import concatenate_datasets

# Gabungkan semuanya
all_data = concatenate_datasets([dtrain, dvalid, dtest])

# Hitung durasi tiap sampel
durations = [len(sample["audio"]["array"]) / sample["audio"]["sampling_rate"] for sample in all_data]

# Hitung rata-rata dan deviasi standar
mean_duration = np.mean(durations)
std_duration = np.std(durations)

# Batas maksimal durasi, misalnya mean + 2 * std
max_duration = mean_duration + 2 * std_duration

print(f"üìè Mean duration: {mean_duration:.2f}s")
print(f"üìè Max duration (trimming threshold): {max_duration:.2f}s")

def compute_duration(example):
    array = example["audio"]["array"]
    sr = example["audio"]["sampling_rate"]
    duration = len(array) / sr
    example["duration"] = duration
    return example

# Tambahkan durasi ke data
dtrain = dtrain.map(compute_duration)
dvalid = dvalid.map(compute_duration)
dtest = dtest.map(compute_duration)

MAX_DURATION = 8.26  # detik

def filter_duration(example):
    return example["duration"] <= MAX_DURATION

dtrain = dtrain.filter(filter_duration)
dvalid = dvalid.filter(filter_duration)
dtest = dtest.filter(filter_duration)

import re

def normalize_text(example):
    text = example["sentence"]

    # Lowercase
    text = text.lower()

    # Hapus tanda baca & simbol kecuali huruf dan spasi
    text = re.sub(r"[^a-zA-Z\s]", "", text)

    # Hilangkan spasi berlebih
    text = re.sub(r"\s+", " ", text).strip()

    example["sentence"] = text
    return example

# Terapkan ke semua split
dtrain = dtrain.map(normalize_text)
dvalid = dvalid.map(normalize_text)
dtest = dtest.map(normalize_text)

dtrain.save_to_disk("/content/common_voice_train_normalized.arrow")
dvalid.save_to_disk("/content/common_voice_valid_normalized.arrow")
dtest.save_to_disk("/content/common_voice_test_normalized.arrow")

import torchaudio
import torch

mel_extractor = torch.nn.Sequential(
    torchaudio.transforms.MelSpectrogram(
        sample_rate=16000,
        n_fft=1024,
        hop_length=256,
        n_mels=80
    ),
    torchaudio.transforms.AmplitudeToDB()
)

def extract_mel(example):
    array = example["audio"]["array"]
    tensor = torch.tensor(array, dtype=torch.float32)

    # Pastikan bentuk tensor [1, time]
    if tensor.ndim == 1:
        tensor = tensor.unsqueeze(0)

    mel_spec = mel_extractor(tensor)
    mel_spec = mel_spec.squeeze(0)  # Buang dimensi channel

    example["mel_spec"] = mel_spec.numpy()
    return example

dtrain = dtrain.map(extract_mel)
dvalid = dvalid.map(extract_mel)
dtest = dtest.map(extract_mel)

dtrain.save_to_disk("/content/common_voice_train_mel.arrow")
dvalid.save_to_disk("/content/common_voice_valid_mel.arrow")
dtest.save_to_disk("/content/common_voice_test_mel.arrow")

from collections import Counter
from itertools import chain

def extract_chars(batch):
    text = "".join(batch["sentence"])
    return {"chars": list(text)}  # ‚úÖ Ini benar

vocab_counter = Counter()

for dataset in [dtrain, dvalid, dtest]:
    chars = dataset.map(extract_chars, batched=True, remove_columns=dataset.column_names)
    vocab_counter.update(chain.from_iterable(chars["chars"]))

# Tambah simbol khusus
vocab = sorted(set(vocab_counter))
vocab_dict = {char: i+1 for i, char in enumerate(vocab)}  # Mulai dari 1
vocab_dict["|"] = vocab_dict.get(" ", len(vocab_dict) + 1)  # Ganti spasi dengan |
vocab_dict["<pad>"] = 0

def encode_text(example):
    chars = list(example["sentence"].lower().replace(" ", "|"))
    example["labels"] = [vocab_dict.get(c, 0) for c in chars]  # 0 untuk unknown
    return example

dtrain = dtrain.map(encode_text)
dvalid = dvalid.map(encode_text)
dtest = dtest.map(encode_text)

import json
with open("/content/vocab.json", "w") as f:
    json.dump(vocab_dict, f)

from google.colab import drive
drive.mount('/content/drive')

dtrain.save_to_disk("/content/drive/MyDrive/dataset_preprocessed/dtrain")
dvalid.save_to_disk("/content/drive/MyDrive/dataset_preprocessed/dvalid")
dtest.save_to_disk("/content/drive/MyDrive/dataset_preprocessed/dtest")

print(len(dtrain))
print(len(dvalid))
print(len(dtest))

dtrain = load_from_disk("/content/common_voice_train_clean.arrow")
dvalid = load_from_disk("/content/common_voice_valid_clean.arrow")
dtest = load_from_disk("/content/common_voice_test_clean.arrow")